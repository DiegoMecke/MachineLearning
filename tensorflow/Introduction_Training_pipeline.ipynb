{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xh-kaHNcfS7s",
    "tags": []
   },
   "source": [
    "# Introduction to Tensorflow : Set training pipeline\n",
    "### By: Diego Coello de Portugal Mecke\n",
    "\n",
    "This notebook aims to create model trainning pipeline without using standard tensorflow functionalities.\n",
    "\n",
    "The data is synthetic (sinuosoidal function) with some outliers for test robustness of the model.\n",
    "A numoy array and a generator will be used to prove the generality of the implemented functionalities.\n",
    "\n",
    "The usage of regularization and gradient clipping will be tested for the case with outliers.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mr3-yF_1XGO7",
    "outputId": "beb1a65e-a4dd-4cb7-f80a-400231776393"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'dtensor' from 'tensorflow.compat.v2.experimental' (C:\\Users\\omen1\\anaconda3\\envs\\dllab\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\experimental\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute_import, division, print_function, unicode_literals\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Flatten, Conv2D\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model, regularizers\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dllab\\lib\\site-packages\\keras\\api\\_v2\\keras\\__init__.py:12\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dllab\\lib\\site-packages\\keras\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dllab\\lib\\site-packages\\keras\\models\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Functional\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dllab\\lib\\site-packages\\keras\\engine\\functional.py:26\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layout_map \u001b[38;5;28;01mas\u001b[39;00m layout_map_lib\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_layer\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dllab\\lib\\site-packages\\keras\\backend.py:34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend_config\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_coordinator_utils \u001b[38;5;28;01mas\u001b[39;00m dc\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtensor_api \u001b[38;5;28;01mas\u001b[39;00m dtensor\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_tensor\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_flow_util\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dllab\\lib\\site-packages\\keras\\dtensor\\__init__.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Conditional import the dtensor API, since it is currently broken in OSS.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _DTENSOR_API_ENABLED:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtensor \u001b[38;5;28;01mas\u001b[39;00m dtensor_api\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Leave it with a placeholder, so that the import line from other python\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# file will not break.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     dtensor_api \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'dtensor' from 'tensorflow.compat.v2.experimental' (C:\\Users\\omen1\\anaconda3\\envs\\dllab\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\experimental\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model, regularizers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import matplotlib.pyplot as plt\n",
    "#from google.colab import drive\n",
    "\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set seeds for reproducibility\n",
    "tf.random.set_seed(24)\n",
    "\n",
    "random_state = 24\n",
    "np.random.state = random_state\n",
    "np.random.seed = random_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bD2az2VXwtx",
    "tags": []
   },
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8w13xdzXtNe"
   },
   "outputs": [],
   "source": [
    "def loadData(n=16000, outliers=False):\n",
    "    \n",
    "    # Get data\n",
    "    x = np.random.rand(n,1)*2*np.pi-np.pi \n",
    "    y = np.sin(x)\n",
    "    \n",
    "    # Add outliers\n",
    "    if outliers:\n",
    "        y[np.random.randint(0,n,3)]=1000.\n",
    "    \n",
    "    return x , y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "GcsafIDr90iE",
    "outputId": "0bd5dee8-00eb-40ac-fde3-b928580c85dc"
   },
   "outputs": [],
   "source": [
    "x,y = loadData()\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYjifVRqbLMr",
    "tags": []
   },
   "source": [
    "## Create a class to define and use model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y0Fv2md1YyKx"
   },
   "outputs": [],
   "source": [
    "# Implementation of simple feedforward network\n",
    "class FCNet(Model):\n",
    "    def __init__(self, neurons=[12,12,3], reg=None,activation=\"relu\"):\n",
    "        super(FCNet, self).__init__()\n",
    "\n",
    "        self.denseLayers=[]\n",
    "        for idx,neuron in enumerate(neurons):\n",
    "            self.denseLayers.append(Dense(neuron, activation=\"relu\"))\n",
    "\n",
    "        self.outputLayer = Dense(1, activation=None)\n",
    "\n",
    "    def call(self, input_x):\n",
    "        output = input_x\n",
    "\n",
    "        for layer in self.denseLayers:\n",
    "            output = layer(output)\n",
    "\n",
    "        return self.outputLayer(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dG8wbG2IbT2_",
    "tags": []
   },
   "source": [
    "## Optimization routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KaVxN-qMaY9v"
   },
   "outputs": [],
   "source": [
    "class TrainModel:\n",
    "\n",
    "    def __init__(self, model, batch_size = 8, lr = 0.001, loss = tf.keras.losses.MeanSquaredError, opt=tf.keras.optimizers.Adam):\n",
    "\n",
    "        self.model      = model\n",
    "        self.loss       = loss()\n",
    "        self.optimizer  = opt(learning_rate = lr)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "        self.test_loss  = tf.keras.metrics.Mean(name='test_loss')\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, x , y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(x) #Update\n",
    "            loss = self.loss(y, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        self.train_loss.update_state(loss)\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, x , y):\n",
    "        predictions = self.model(x)\n",
    "        loss = self.loss(y, predictions)\n",
    "        self.test_loss.update_state(loss)\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        loss = []\n",
    "        for bX, bY in self.train_ds:\n",
    "            loss.append(self.train_step(bX, bY))\n",
    "        return loss\n",
    "\n",
    "    def test(self):\n",
    "        loss = []\n",
    "        for bX, bY in self.test_ds:\n",
    "            loss.append(self.test_step(bX, bY))  \n",
    "        return loss \n",
    "\n",
    "    def run(self, dataX, dataY, testX, testY, epochs, verbose=1):\n",
    "        history = []\n",
    "\n",
    "        self.train_ds = tf.data.Dataset.from_tensor_slices((dataX, dataY)).shuffle(16000).batch(self.batch_size)\n",
    "        self.test_ds  = tf.data.Dataset.from_tensor_slices((testX,testY)).batch(self.batch_size)\n",
    "\n",
    "        for i in range(epochs):\n",
    "\n",
    "            train_loss = self.train()\n",
    "            test_loss  = self.test()\n",
    "\n",
    "            history.append([train_loss,test_loss])\n",
    "\n",
    "            if verbose > 0 and (i==0 or (i+1)%5==0):\n",
    "                \n",
    "                print(f\"epoch: {i+1}, TRAIN LOSS: {self.train_loss.result()}, TEST LOSS: {self.test_loss.result()}\")\n",
    "\n",
    "                self.train_loss.reset_states()\n",
    "                self.test_loss.reset_states()\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gH38m2ykb18C",
    "outputId": "8b27baf8-36cc-44c6-d557-3205721319bb"
   },
   "outputs": [],
   "source": [
    "x_train, y_train = loadData()\n",
    "x_test, y_test  = loadData()\n",
    "\n",
    "model = FCNet()\n",
    "opt   = TrainModel(model, batch_size=8, lr=0.001, loss=tf.keras.losses.MeanSquaredError, opt=tf.keras.optimizers.SGD)\n",
    "\n",
    "hist = opt.run(x_train, y_train, x_test, y_test, epochs=20, verbose=1)\n",
    "hist = np.array(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "8viY-rISRL4o",
    "outputId": "55e09097-c865-4d6c-c216-d4f2b8a171e7"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.mean(hist,-1)[:,0],label=\"train\")\n",
    "plt.plot(np.mean(hist,-1)[:,1],label=\"test\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Training/Test Loss Convergence\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqRlbPI6xvsj"
   },
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5KSZ8fCofS71"
   },
   "outputs": [],
   "source": [
    "def genData(batch_size=100):\n",
    "    while True:\n",
    "        x = np.random.rand(batch_size,1)*2*np.pi-np.pi \n",
    "        y = np.sin(x)\n",
    "        yield x , y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJQ-ghf-fS72"
   },
   "outputs": [],
   "source": [
    "#Definition of RunModel class (variation of TrainClass with new run method)\n",
    "class RunModel:\n",
    "\n",
    "    def __init__(self, model, batch_size=8, lr=0.001, loss=tf.keras.losses.MeanSquaredError, opt=tf.keras.optimizers.Adam):\n",
    "\n",
    "        self.model      = model\n",
    "        self.loss       = loss()\n",
    "        self.optimizer  = opt(learning_rate = lr)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        self.test_loss  = tf.keras.metrics.Mean(name='test_loss')\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, x , y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            inp = tf.convert_to_tensor([x], dtype=tf.float64)\n",
    "            out = tf.convert_to_tensor([y], dtype=tf.float64)\n",
    "            predictions = model(inp)\n",
    "            loss = self.loss(out, predictions)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        self.train_loss.update_state(loss)\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, x , y):\n",
    "        predictions = self.model(x)\n",
    "        loss = self.loss(y, predictions)\n",
    "        self.test_loss.update_state(loss)\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        loss = []\n",
    "        for bX, bY in self.train_ds:\n",
    "            loss.append(self.train_step(bX, bY))\n",
    "        return loss\n",
    "\n",
    "    def test(self):\n",
    "        loss = []\n",
    "        for bX, bY in self.test_ds:\n",
    "            loss.append(self.test_step(bX, bY))  \n",
    "        return loss \n",
    "\n",
    "    def run(self, trainGen, testGen, epochs, verbose=2): #Update\n",
    "        history = []\n",
    "        batch_size = (next(trainGen)[0]).shape[0] #Update\n",
    "\n",
    "        for i in range(epochs):\n",
    "            self.train_ds = tf.data.Dataset.from_tensor_slices(next(train_gen)).batch(batch_size) #Update\n",
    "            self.test_ds  = tf.data.Dataset.from_tensor_slices(next(test_gen)).batch(batch_size) #Update\n",
    "\n",
    "#             self.train_ds = tf.data.Dataset.from_generator(train_gen)\n",
    "#             self.test_ds  = tf.data.Dataset.from_generator(test_gen)\n",
    "\n",
    "            train_loss = self.train()\n",
    "            test_loss  = self.test()\n",
    "\n",
    "            history.append([train_loss,test_loss])\n",
    "\n",
    "            if verbose > 0 and (i==0 or (i+1)%5==0):\n",
    "                print(f\"epoch: {i+1}, TRAIN LOSS: {self.train_loss.result()}, TEST LOSS: {self.test_loss.result()}\")\n",
    "\n",
    "                self.train_loss.reset_states()\n",
    "                self.test_loss.reset_states()\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vM9IGDQRfS72",
    "outputId": "ffa9041c-dc93-4dfa-d508-c5813f980975"
   },
   "outputs": [],
   "source": [
    "train_gen = genData(batch_size=16000)\n",
    "test_gen  = genData(batch_size=16000)\n",
    "\n",
    "# Running this requires to update RunModel\n",
    "model  = FCNet()\n",
    "opt    = RunModel(model, lr=0.001, loss=tf.keras.losses.MeanSquaredError, opt=tf.keras.optimizers.SGD)\n",
    "\n",
    "hist = opt.run(train_gen, test_gen, 30, verbose=1)\n",
    "hist = np.array(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.mean(hist,-1)[:,0],label=\"train\")\n",
    "plt.plot(np.mean(hist,-1)[:,1],label=\"test\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Training/Test Loss Convergence\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result difference between the generator and the numpy case are due to the difference in data. One epoch of the generator has 8 instances, while the generator has 16000 instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCApVgacawzh"
   },
   "source": [
    "## Test L2 regularization and gradient clipping by norm to the model.\n",
    "\n",
    "Test implementations by running the original code with outliers in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eE_Oe0BKrBxY"
   },
   "outputs": [],
   "source": [
    "xn_train, yn_train = loadData(outliers=True)\n",
    "x_test, y_test   = loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xn_train, yn_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GN4kkO5fS72"
   },
   "source": [
    "Add regularization in the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAg3F124fS72"
   },
   "outputs": [],
   "source": [
    "# Implementation of simple feedforward network with regularization\n",
    "class FCNet2(Model):\n",
    "    def __init__(self, neurons=[12,12,3], reg=0.001, activation=\"relu\"): #Update\n",
    "        super(FCNet2, self).__init__()\n",
    "\n",
    "        self.denseLayers=[]\n",
    "        for idx,neuron in enumerate(neurons):\n",
    "            self.denseLayers.append(Dense(neuron, kernel_regularizer=regularizers.l2(reg), activation=\"relu\")) #Update\n",
    "\n",
    "        self.outputLayer = Dense(1, kernel_regularizer=regularizers.l2(reg), activation=None)\n",
    "\n",
    "    def call(self, input_x):\n",
    "        output = input_x\n",
    "\n",
    "        for layer in self.denseLayers:\n",
    "            output = layer(output)\n",
    "\n",
    "        return self.outputLayer(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUdbwj3sfS72"
   },
   "source": [
    "Adding clipvalue to the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbG-Ply1fS73"
   },
   "outputs": [],
   "source": [
    "class TrainModel2:\n",
    "\n",
    "    def __init__(self, model, batch_size=8, lr=0.001, loss=tf.keras.losses.MeanSquaredError, opt=tf.keras.optimizers.Adam,\\\n",
    "                clip_value=1.0):\n",
    "\n",
    "        self.model      = model\n",
    "        self.loss       = loss()\n",
    "        self.optimizer  = opt(learning_rate=lr, clipvalue=clip_value) #Update\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "        self.test_loss  = tf.keras.metrics.Mean(name='test_loss')\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, x , y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(x)\n",
    "            loss = self.loss(y, predictions) + sum(self.model.losses)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients,self.model.trainable_variables))\n",
    "        self.train_loss.update_state(loss)\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, x , y):\n",
    "        predictions = self.model(x)\n",
    "        loss = self.loss(y, predictions)\n",
    "        self.test_loss.update_state(loss)\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        loss = []\n",
    "        for bX, bY in self.train_ds:\n",
    "            loss.append(self.train_step(bX, bY))\n",
    "        return loss\n",
    "\n",
    "    def test(self):\n",
    "        loss = []\n",
    "        for bX, bY in self.test_ds:\n",
    "            loss.append(self.test_step(bX, bY))  \n",
    "        return loss \n",
    "\n",
    "    def run(self, dataX, dataY, testX, testY, epochs, verbose=2):\n",
    "        history = []\n",
    "\n",
    "        self.train_ds = tf.data.Dataset.from_tensor_slices((dataX, dataY)).shuffle(16000).batch(self.batch_size)\n",
    "        self.test_ds  = tf.data.Dataset.from_tensor_slices((testX,testY)).batch(self.batch_size)\n",
    "\n",
    "        for i in range(epochs):\n",
    "\n",
    "            train_loss = self.train()\n",
    "            test_loss  = self.test()\n",
    "\n",
    "            history.append([train_loss,test_loss])\n",
    "\n",
    "            if verbose > 0 and (i==0 or (i+1)%5==0):\n",
    "                print(f\"epoch: {i+1}, TRAIN LOSS: {self.train_loss.result()}, TEST LOSS: {self.test_loss.result()}\")\n",
    "\n",
    "\n",
    "                self.train_loss.reset_states()\n",
    "                self.test_loss.reset_states()\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCNet()\n",
    "\n",
    "opt = TrainModel(model, batch_size=8, lr=0.001, loss=tf.keras.losses.MeanSquaredError, opt=tf.keras.optimizers.Adam)\n",
    "\n",
    "hist = opt.run(xn_train, yn_train, x_test, y_test, 30, verbose=1)\n",
    "hist = np.array(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "print('------Base model for outliers------')\n",
    "\n",
    "axs[0].plot(np.mean(hist,-1)[:,0],label=\"train\")\n",
    "axs[0].set_title(\"Training Loss Convergence\")\n",
    "axs.flat[0].set(xlabel='Epochs', ylabel='MSE')\n",
    "\n",
    "axs[1].plot(np.mean(hist,-1)[:,1],label=\"test\")\n",
    "axs[1].set_title(\"Test Loss Convergence\")\n",
    "axs.flat[1].set(xlabel='Epochs', ylabel='MSE')\n",
    "\n",
    "axs[2].scatter(x_test,y_test,label=\"true\")\n",
    "axs[2].scatter(x_test, model(x_test),label=\"pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model result with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i_Gjl8SprAzy",
    "outputId": "9b0c84c8-c8ca-45d6-d84b-1ef68f9a1e3c"
   },
   "outputs": [],
   "source": [
    "model = FCNet2(reg=0.001)\n",
    "\n",
    "opt = TrainModel(model, batch_size=8, lr=0.001, loss=tf.keras.losses.MeanSquaredError, opt=tf.keras.optimizers.Adam)\n",
    "\n",
    "hist = opt.run(xn_train, yn_train, x_test, y_test, 30, verbose=1)\n",
    "hist = np.array(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "afQYwwGmfS73",
    "outputId": "8bc44aa2-1965-48ce-ede2-23dd14e79a3b"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "print('------Model result with regularization for outliers------')\n",
    "\n",
    "axs[0].plot(np.mean(hist,-1)[:,0],label=\"train\")\n",
    "axs[0].set_title(\"Training Loss Convergence\")\n",
    "axs.flat[0].set(xlabel='Epochs', ylabel='MSE')\n",
    "\n",
    "axs[1].plot(np.mean(hist,-1)[:,1],label=\"test\")\n",
    "axs[1].set_title(\"Test Loss Convergence\")\n",
    "axs.flat[1].set(xlabel='Epochs', ylabel='MSE')\n",
    "\n",
    "axs[2].scatter(x_test,y_test,label=\"true\")\n",
    "axs[2].scatter(x_test, model(x_test),label=\"pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This specific seed doesn't return very good results for this setting, but in general it should be a little bit better than the base model. The test loss plot can be seen as a prove of this, since there are previous epochs with better performance than the last iteration (from which we take the final loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model trainning with gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCNet()\n",
    "\n",
    "opt = TrainModel2(model, batch_size=8, lr=0.001, loss=tf.keras.losses.MeanSquaredError, opt=tf.keras.optimizers.Adam,\\\n",
    "                clip_value=2.0)\n",
    "\n",
    "hist = opt.run(xn_train, yn_train, x_test, y_test, 30, verbose=1)\n",
    "hist = np.array(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "print('------Model result with gradient clipping for outliers------')\n",
    "\n",
    "axs[0].plot(np.mean(hist,-1)[:,0],label=\"train\")\n",
    "axs[0].set_title(\"Training Loss Convergence\")\n",
    "axs.flat[0].set(xlabel='Epochs', ylabel='MSE')\n",
    "\n",
    "axs[1].plot(np.mean(hist,-1)[:,1],label=\"test\")\n",
    "axs[1].set_title(\"Test Loss Convergence\")\n",
    "axs.flat[1].set(xlabel='Epochs', ylabel='MSE')\n",
    "\n",
    "axs[2].scatter(x_test,y_test,label=\"true\")\n",
    "axs[2].scatter(x_test, model(x_test),label=\"pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model trainning with gradient clipping and regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCNet2(reg=0.001)\n",
    "\n",
    "opt = TrainModel2(model, batch_size=8, lr=0.001, loss=tf.keras.losses.MeanSquaredError, opt=tf.keras.optimizers.Adam,\\\n",
    "                clip_value=2.0)\n",
    "\n",
    "hist = opt.run(xn_train, yn_train, x_test, y_test, 30, verbose=1)\n",
    "hist = np.array(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,3,figsize=(15,5))\n",
    "\n",
    "print('------Model result with regularization and gradient clipping for outliers------')\n",
    "\n",
    "axs[0].plot(np.mean(hist,-1)[:,0],label=\"train\")\n",
    "axs[0].set_title(\"Training Loss Convergence\")\n",
    "axs.flat[0].set(xlabel='Epochs', ylabel='MSE')\n",
    "\n",
    "axs[1].plot(np.mean(hist,-1)[:,1],label=\"test\")\n",
    "axs[1].set_title(\"Test Loss Convergence\")\n",
    "axs.flat[1].set(xlabel='Epochs', ylabel='MSE')\n",
    "\n",
    "axs[2].scatter(x_test,y_test,label=\"true\")\n",
    "axs[2].scatter(x_test, model(x_test), label=\"pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and conclusions:\n",
    "\n",
    "| *Loss* | Clipping | No clipping |\n",
    "| --- | --- | --- |\n",
    "| Regularization | **0.0004** | 0.03885 |\n",
    "| No Regularization | 0.00148 | 0.05 |\n",
    "\n",
    "After some quick testing for hyperparameter tunning, regularization doesn't seem to have such a big influence for performance compared to gradient clipping.\n",
    "\n",
    "This is consistent to the idea that regularization aims to avoid overfitting, while gradient clipping is more suitable to avoid the model diverging due to poissoned data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5TczlcEZ_S4"
   },
   "source": [
    "## Rewriting the code to different files according to their names and purpose:\n",
    "\n",
    "- data_loading.py\n",
    "- model.py\n",
    "- train.py\n",
    "- run.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception('STOP CELL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When using google drive\n",
    "from google.colab import drive\n",
    "import sys\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "sys.path.append('/content/gdrive/My Drive/') # CHANGE THIS LINE DEPENDING OF WHERE YOU PUT YOUR FILES IN GOOGLE DRIVE\n",
    "\n",
    "from run import run_experiment\n",
    "run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "id": "qXhfGOLpUCuW",
    "outputId": "b7b76bba-0e52-4e99-92cb-facaa2bde531"
   },
   "outputs": [],
   "source": [
    "#When loading from local folder\n",
    "from python_files.run import run_experiment\n",
    "run_experiment()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
